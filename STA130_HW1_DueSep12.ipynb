{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1cbc3a",
   "metadata": {},
   "source": [
    "QUESTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d79bfc",
   "metadata": {},
   "source": [
    "I decided to just import the dataset from the further guidance section under question 1, since I couldn't find the tutorial summary for TUT208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd87b1f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db476d4",
   "metadata": {},
   "source": [
    "ChatGPT wasn't able to check the code for missing values, so instead it gave me this code that would check for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92212833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_n           0\n",
      "id              1\n",
      "name            0\n",
      "gender          0\n",
      "species         0\n",
      "birthday        0\n",
      "personality     0\n",
      "song           11\n",
      "phrase          0\n",
      "full_id         0\n",
      "url             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = villagers_df.isnull().sum()\n",
    "\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088cece",
   "metadata": {},
   "source": [
    "And this code does show that there are in fact a couple of missing values in some of the sections. Also the first bit of code was adequate for importing the dataset, but it was redone in the most recent lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5e3d9",
   "metadata": {},
   "source": [
    "QUESTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34ff62",
   "metadata": {},
   "source": [
    "I got the code necessary for this question from chatGPT, telling me exactly how many rows and columns there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb52a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 391 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = villagers_df.shape\n",
    "\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d4c6e",
   "metadata": {},
   "source": [
    "I then asked chatGPT about what \"observations\" and \"variables\" mean in the context of this dataset. ChatGPT told me that variables in this context mean the names of the columns, and that observations are the rows that that go down from them. Basically the observations are seperate entries of data, meanwhile variables are the different attributes that are recorded per observation in the data set. So observations are the rows and variables are the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bf9b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "QUESTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c9aa9",
   "metadata": {},
   "source": [
    " I asked ChatGPT to write me code that would allow me to summarise the columns of the dataset. Unforunately this first code generated an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3995793e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m numerical_summary \u001b[38;5;241m=\u001b[39m villagers_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Provide value counts for a categorical column (replace 'column' with the actual column name)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m categorical_summary \u001b[38;5;241m=\u001b[39m \u001b[43mvillagers_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumerical Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'column'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Provide a summary for numerical columns\n",
    "numerical_summary = villagers_df.describe()\n",
    "\n",
    "# Provide value counts for a categorical column (replace 'column' with the actual column name)\n",
    "categorical_summary = villagers_df['column'].value_counts()\n",
    "\n",
    "# Display the results\n",
    "print(\"Numerical Summary:\")\n",
    "print(numerical_summary)\n",
    "\n",
    "print(\"\\nValue Counts for a specific column:\")\n",
    "print(categorical_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4dd3db",
   "metadata": {},
   "source": [
    "I then put the error into chatGPT and got told to try this new code instead, but I needed to replace 'Name' with the name of an actual column that I watned to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74594e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['row_n', 'id', 'name', 'gender', 'species', 'birthday', 'personality',\n",
      "       'song', 'phrase', 'full_id', 'url'],\n",
      "      dtype='object')\n",
      "\n",
      "Numerical Summary:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "\n",
      "Value Counts for 'Name':\n",
      "song\n",
      "K.K. Country     10\n",
      "Forest Life       9\n",
      "Imperial K.K.     7\n",
      "K.K. Soul         7\n",
      "K.K. Ragtime      7\n",
      "                 ..\n",
      "Aloha K.K.        2\n",
      "Drivin'           1\n",
      "Senor K.K.        1\n",
      "K.K.  Bazaar      1\n",
      "K.K. D&B          1\n",
      "Name: count, Length: 92, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Display the column names to see what's available\n",
    "print(\"Columns in the dataset:\")\n",
    "print(villagers_df.columns)\n",
    "\n",
    "# Provide a summary for numerical columns\n",
    "numerical_summary = villagers_df.describe()\n",
    "\n",
    "# Provide value counts for a specific categorical column (replace 'Name' with any column name you want to analyze)\n",
    "categorical_summary = villagers_df['song'].value_counts()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nNumerical Summary:\")\n",
    "print(numerical_summary)\n",
    "\n",
    "print(\"\\nValue Counts for 'Name':\")\n",
    "print(categorical_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3e0f3",
   "metadata": {},
   "source": [
    "While that did let me analyze one column, I would need to manually change the name if I wanted to look at a different column, so I asked chatGPT to modify the code so that it would give me the summaries of all the columns at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d9dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Summary:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "\n",
      "Categorical Summaries:\n",
      "\n",
      "Value Counts for 'id':\n",
      "id\n",
      "admiral    1\n",
      "mott       1\n",
      "paula      1\n",
      "patty      1\n",
      "pate       1\n",
      "          ..\n",
      "eloise     1\n",
      "elmer      1\n",
      "ellie      1\n",
      "elise      1\n",
      "zucker     1\n",
      "Name: count, Length: 390, dtype: int64\n",
      "\n",
      "Value Counts for 'name':\n",
      "name\n",
      "Admiral    1\n",
      "Muffy      1\n",
      "Paula      1\n",
      "Patty      1\n",
      "Pate       1\n",
      "          ..\n",
      "Elvis      1\n",
      "Eloise     1\n",
      "Elmer      1\n",
      "Ellie      1\n",
      "Zucker     1\n",
      "Name: count, Length: 391, dtype: int64\n",
      "\n",
      "Value Counts for 'gender':\n",
      "gender\n",
      "male      204\n",
      "female    187\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for 'species':\n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for 'birthday':\n",
      "birthday\n",
      "1-27     2\n",
      "12-5     2\n",
      "7-31     2\n",
      "3-26     2\n",
      "8-3      2\n",
      "        ..\n",
      "4-3      1\n",
      "10-26    1\n",
      "7-23     1\n",
      "12-8     1\n",
      "3-8      1\n",
      "Name: count, Length: 361, dtype: int64\n",
      "\n",
      "Value Counts for 'personality':\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for 'song':\n",
      "song\n",
      "K.K. Country     10\n",
      "Forest Life       9\n",
      "Imperial K.K.     7\n",
      "K.K. Soul         7\n",
      "K.K. Ragtime      7\n",
      "                 ..\n",
      "Aloha K.K.        2\n",
      "Drivin'           1\n",
      "Senor K.K.        1\n",
      "K.K.  Bazaar      1\n",
      "K.K. D&B          1\n",
      "Name: count, Length: 92, dtype: int64\n",
      "\n",
      "Value Counts for 'phrase':\n",
      "phrase\n",
      "wee one       2\n",
      "quacko        2\n",
      "bloop         2\n",
      "aye aye       1\n",
      "snoot         1\n",
      "             ..\n",
      "lambchop      1\n",
      "yeah buddy    1\n",
      "chow down     1\n",
      "unh-hunh      1\n",
      "pronk         1\n",
      "Name: count, Length: 388, dtype: int64\n",
      "\n",
      "Value Counts for 'full_id':\n",
      "full_id\n",
      "villager-admiral    1\n",
      "villager-muffy      1\n",
      "villager-paula      1\n",
      "villager-patty      1\n",
      "villager-pate       1\n",
      "                   ..\n",
      "villager-elvis      1\n",
      "villager-eloise     1\n",
      "villager-elmer      1\n",
      "villager-ellie      1\n",
      "villager-zucker     1\n",
      "Name: count, Length: 391, dtype: int64\n",
      "\n",
      "Value Counts for 'url':\n",
      "url\n",
      "https://villagerdb.com/images/villagers/thumb/admiral.98206ee.png    1\n",
      "https://villagerdb.com/images/villagers/thumb/muffy.1497c92.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/paula.563ba81.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/patty.3e17f7f.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/pate.c60838c.png       1\n",
      "                                                                    ..\n",
      "https://villagerdb.com/images/villagers/thumb/elvis.57d4757.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/eloise.112208b.png     1\n",
      "https://villagerdb.com/images/villagers/thumb/elmer.cc7df52.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/ellie.5a144a6.png      1\n",
      "https://villagerdb.com/images/villagers/thumb/zucker.8dbb719.png     1\n",
      "Name: count, Length: 391, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Provide a summary for numerical columns\n",
    "numerical_summary = villagers_df.describe()\n",
    "\n",
    "# Identify categorical columns (non-numerical)\n",
    "categorical_columns = villagers_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Provide value counts for all categorical columns\n",
    "categorical_summary = {}\n",
    "for column in categorical_columns:\n",
    "    categorical_summary[column] = villagers_df[column].value_counts()\n",
    "\n",
    "# Display the results\n",
    "print(\"Numerical Summary:\")\n",
    "print(numerical_summary)\n",
    "\n",
    "print(\"\\nCategorical Summaries:\")\n",
    "for column, value_counts in categorical_summary.items():\n",
    "    print(f\"\\nValue Counts for '{column}':\")\n",
    "    print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231483f",
   "metadata": {},
   "source": [
    "QUESTION 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81362df5",
   "metadata": {},
   "source": [
    "Since I used the data that was did not have (a) non-numeric variables and (b) missing values in numeric variables. For this reason I will be using the titanic dataset provided under the further guidance section to answer this question. \n",
    "\n",
    "So I asked ChatGPT To help me explain the problem, and am now going to use my own words to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49528ebd",
   "metadata": {},
   "source": [
    "If we compare the number of columns analyzed by .describe() compared to .shape, the reasont that we may end up getting two different answers, is because .shape looks at the whole dataset and analyzes the total number of rows and columns in the dataset. Meanwhile the .describe() only summarizes numerical data, and ignores categorical data unless specified otherwise by passing the include = \"all\" argument through .describe()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74c550",
   "metadata": {},
   "source": [
    "When looking at the values reported in the \"count\" column, .shape gives the total number of rows in the column, regardless of if those rows actually contain any data. On the other hand, .describe() would give the total number of non-missing values, so if the \"count\" column has a couple of missing values then .describe() would just ignore and pass over them. Which explains the discrepancies between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7fefe",
   "metadata": {},
   "source": [
    "Since this question was about explanation and not code, there is no code for me to present, either from ChatGPT or myself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95173ae6",
   "metadata": {},
   "source": [
    "QUESTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7e40e",
   "metadata": {},
   "source": [
    "After consulting with ChatGPT, I have figured out the difference between attributes and methods.\n",
    "Attributes are just the storage of data or metadata, it is a part of the object that doesn't actually preform any calculations or anything. When an attribute is called, all that's being done is the data of the object is being accessed. The reason that there aren't brackets at the end of an attribute, is because there isn't any input needed to return the right value.\n",
    "Meanwhile methods are functions that simply belong to an object. Unlike attributes, methods actually do actions and the reason that they have brackets at the end, is because they are executing a function which always have brackets at the end just in case someone needs to pass an argument through them, so the brackets are needed at the end even if nothing is actually being passed through them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6330b",
   "metadata": {},
   "source": [
    "QUESTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309c962",
   "metadata": {},
   "source": [
    "I know what all of these terms mean so ChatGPT wasn't used for this question.\n",
    "count: The number of values that aren't missing for a particular variable of the dataset.\n",
    "mean: The total sum of the data points for each variable devided by the total amount of data points to get the average value.\n",
    "std: Helps to quantify how spread out or close the data is, specifically in relation to the mean.\n",
    "min: The smallest overall value in a variable.\n",
    "max: The largest overall value in a variable.\n",
    "25%: 25% represents the first quartile and it means that 25% of the data is under this point, while 75% of the data is above.\n",
    "50%: Also known as the median, it's the middle of the data, with half the data being under this point and half the data being over.\n",
    "75%: 75% represents the third quartile and it means that 75% of the data is under this point, while 25% of the data is above.\n",
    "\n",
    "And naturally all of this data is only for numeric variables, because categorical variables can't be quantized like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a9550",
   "metadata": {},
   "source": [
    "QUESTION 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc83b2",
   "metadata": {},
   "source": [
    "1) I asked chatGPT to provide me with a situation where .dropna() might be better than using del ['col']. And what ChatGPT told me is that .dropna(), deletes only the rows or columns with missing values, while del ['col'] ends up deleting a whole column regardless of if there are missing values or not. .dropna() offers more control while del ['col'] is just used for removing unwatned columns. For example, using the titanic dataset, if I only want to know about the passangers who's ages were known I would use .dropna() to get rid of all the passangers who's ages were unknown. Deleting the column wouldn't accomplish anything in this case but .dropna() would.\n",
    "2) The opposite case once again with the titanic dataset, if the \"Cabin\" column has a lot of missing data and even when the data is there it has nothing to do with the analysis, so to avoid the \"Cabin\" column being annoying even when it provides no actual useful data, we would just use del ['col'] to get rid of the entire column.\n",
    "3) Applying the del ['col'] before the .dropna() especially if you're going to end up using both is important just because deleting a column first, is a good idea since if a column has a lot of missing values and the column itself doesn't have any information that seems particularily necessary for the analysis, deleting the problematic column could save many additional rows from being unnecessarily deleted. \n",
    "4) I asked ChatGPT to provide me with some code that would do exactly what I said in part 2 of this question. But it seems that the Cabin column had already gotten deleted before, so I asked ChatGPT to delete another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e3395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dropa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2435025515.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Embarked' column not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Step 2: Remove rows that still have missing values in the remaining columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtitanic_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Check the cleaned dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dropa'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Check the column names\n",
    "print(titanic_df.columns)\n",
    "\n",
    "# Step 1: Remove the 'Embarked' column\n",
    "if 'embarked' in titanic_df.columns:\n",
    "    del titanic_df['embarked']\n",
    "else:\n",
    "    print(\"'Embarked' column not found\")\n",
    "\n",
    "# Step 2: Remove rows that still have missing values in the remaining columns\n",
    "titanic_cleaned = titanic_df.dropna()\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(titanic_cleaned.shape)\n",
    "print(titanic_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c22a45f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'survived' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msurvived\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFare\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'survived' is not defined"
     ]
    }
   ],
   "source": [
    "titanic_df.groupby(survived)[\"Fare\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9e767",
   "metadata": {},
   "source": [
    "QUESTION 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5a5a5",
   "metadata": {},
   "source": [
    "1) I asked ChatGPT what the differences what, and based on my understanding, the line of code asked about, will first group all of the information from col1 into as many groups as it can, and then it only actually chooses the information from col2 based off of the groups predetermined by col1. And all of that combined with .describe() just means that the data from col2 will be summarised based on its groupings decided by col1.\n",
    "2) After consulting with ChatGPT, the main differences between the .describe() and the .groupby(\"col1\")[\"col2\"].describe() is that the global .describe() gives the general description of how much data is missing per column, and how said missing values affect the overall number of actual data in the column. Meanwhile the more specific .describe() would give data on how the smaller groups are affected by the missing values within each particular group for the data in col2.\n",
    "3) \n",
    "A. Putting in the whole error into ChatGPT got me a very quick response, while putting the whole error into my search engine pulled up a bunch of forums that proved much slower in helping my figure out what the right answer was because it involved me clicking on a bunch of links and scrolling around. \n",
    "B. Same thing as before putting in the whole error into my browser, gave a pretty long winded response while ChatGPT immediately and correctly identified the error as the url being correct, it also remembered the correct url from our previous conversation and provided it to me, something my browser cannot do.\n",
    "C. Here my browser actually managed to figure out the issue faster, but it wasn't able to offer me up a solution other than looking through my code and trying to diagnose the issue meanwhile ChatGPT took longer to write up a full response but it did manage to tell me how to fix the issue. \n",
    "D. This was kind of a tie with both my browser and ChatGpt providing me with a correct answer nearly right away, and since it was a pretty easy fix there wasn't really much to evaluate between what provided a faster answer.\n",
    "E. This was also a win for ChatGPT, which provided me with a much faster without having to click through a bunch of articles, where ChatGPT gave me the exact fix for the code as well. \n",
    "F. For both of these, the error was complicated enough that when I put my question into ChatGPT, it responded with a much faster (and correct) answer, whereas when putting those answers into my browser I still had to look through a link that led to a webpage that no longer existed, I also had to wait for the websites to load and actually figure out what I was looking at in every website.\n",
    "G. Once again, ChatGPT gave me the faster and better answer when compared to my web browser, although it was pretty close and the first link I clicked on and the first words I saw actually did show me the answer I was looking for, so that might be blamed on my internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11be75",
   "metadata": {},
   "source": [
    "QUESTION 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d853d77",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52743b63",
   "metadata": {},
   "source": [
    "SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e3036",
   "metadata": {},
   "source": [
    "Here’s a summary of the entire conversation:\n",
    "\n",
    "You started by asking for help analyzing the Titanic dataset and understanding the difference between methods and attributes in pandas. You wanted to explain discrepancies in dataset size as reported by .shape and .describe() and learn when .dropna() would be preferable to deleting columns directly with del. We discussed specific use cases, such as when .dropna() might be useful in handling missing values across rows, while del is better suited for removing entire columns.\n",
    "\n",
    "We then moved on to issues related to the code you were writing. You encountered errors like FileNotFoundError and syntax errors like NameError and AttributeError. We corrected the case of column names (e.g., using Cabin instead of cabin and fare instead of Fare). There were additional issues with unclosed parentheses and incorrect method names (such as dropa() instead of dropna()).\n",
    "\n",
    "You also ran into problems while using .groupby() with the Titanic dataset. The main issues were related to incorrect case sensitivity in column names (e.g., survived vs. Survived), which caused KeyError and NameError exceptions. We eventually resolved these by ensuring consistent naming conventions in column access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812ff72",
   "metadata": {},
   "source": [
    "CHATGPT LINK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e14907",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/dd856b89-0f4b-447d-918c-ec247a4a411d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
